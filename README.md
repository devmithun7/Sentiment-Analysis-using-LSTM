# ğŸ“˜ Sentiment Analysis on Amazon Customer Reviews Using Parallel Deep Learning

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9%2B-red)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)
[![GitHub Stars](https://img.shields.io/github/stars/devmithun7/Sentiment-Analysis-using-LSTM)](https://github.com/devmithun7/Sentiment-Analysis-using-LSTM/stargazers)

---

## ğŸ“‹ Table of Contents
- [Project Overview](#project-overview)
- [Project Scope](#project-scope)
- [Dataset](#dataset)
- [Technical Architecture](#technical-architecture)
- [Experiments & Methodology](#experiments--methodology)
- [Parallel Processing Techniques](#parallel-processing-techniques)
- [Results & Analysis](#results--analysis)
- [Repository Structure](#repository-structure)
- [Getting Started](#getting-started)
- [Key Findings](#key-findings)
- [References](#references)

---

## ğŸ¯ Project Overview
This project implements scalable sentiment analysis on **3.5M+ Amazon customer reviews** using parallel data processing and distributed deep learning.  
Techniques such as **Distributed Data Parallelism (DDP)**, **model parallelism**, and **Dask-based preprocessing** were evaluated to measure their impact on **training time, speedup, and efficiency**.

The goal is to understand how parallel computing techniques accelerate large-scale NLP workloads and optimize resource utilization.
(Details sourced from project PDF) 

---

## ğŸ”¬ Project Scope

### **1. Enhance Data Processing Efficiency**
- Parallel ingestion using **Dask**
- Clean and standardize review text (lowercasing, removing non-alphabetic chars)
- Convert multi-class review ratings into binary sentiment labels
- Repartition data for balanced worker workloads
- Generate performance reports (task stream, worker profiles, bandwidth)

### **2. Improve Scalability & Resource Utilization**
- Implement **DistributedDataParallel (DDP)** across multiple CPUs
- Compare scaling behavior for 8, 16, 20, 24, and 28 CPU configurations
- Evaluate communication overhead and bottlenecks

### **3. Optimize Deep Learning Model Training**
- Compare **LSTM** and **GRU**, selecting LSTM for best performance
- Build distributed data loaders with `DistributedSampler`
- Implement model saving and epoch-level evaluation

### **4. Benchmark Performance & Scalability**
- Measure training time reduction with increasing CPUs
- Compute speedup and efficiency metrics
- Evaluate how model parallelism affects performance

---

## ğŸ“Š Dataset

### **Dataset Source**
- Amazon Customer Reviews dataset (Kaggle)
- Two files:  
  - `main_data` (1.5 GB)  
  - `subset_data` (163 MB)

### **Dataset Characteristics**
- Total Records: **3.5M+**
- Labels formatted for fastText:
  - `__label__1` â†’ Negative (1â€“2 stars)  
  - `__label__2` â†’ Positive (4â€“5 stars)
- Neutral (3-star) reviews excluded
- No missing values
- Final Split:
  - **3,600,000** training samples  
  - **400,000** test samples  

### **Preprocessing Pipeline**
- Lowercasing text
- Removing non-alphabetic characters
- Stripping unnecessary whitespace
- Partitioning via Dask for parallel efficiency
- Converting labels into binary sentiment

---

## ğŸ—ï¸ Technical Architecture

### **Model: LSTM-Based Sentiment Classifier**
- Word embeddings
- LSTM encoder
- Fully connected layer with sigmoid output
- Loss: Binary Cross-Entropy
- Optimizer: Adam

### **Distributed Training Setup**
- Backend: **gloo** (CPU)
- Multi-process execution using `torch.multiprocessing.spawn`
- Distributed DataLoader using `DistributedSampler`
- Parallel saving of checkpoints

### **Model Parallel Configuration**
- Embedding + FC on `device0`
- LSTM on `device1`
- Forward pass flow:
  1. Embedding â†’ device0  
  2. LSTM â†’ device1  
  3. FC layer â†’ device0  

---

## ğŸ§ª Experiments & Methodology

### **Training Configurations**
- Epochs: 20
- Batch sizes: distributed across CPU cores
- Hardware: multi-core CPU nodes
- DDP runs: 8, 16, 20, 24, 28 CPUs
- Model Parallel + DDP runs: multiple CPU splits

### **Experimental Goals**
- Identify optimal CPU count for training
- Measure diminishing returns at high CPU counts
- Compare pure DDP vs DDP + Model Parallelism

---

## âš¡ Parallel Processing Techniques

### **1ï¸âƒ£ Distributed Data Parallel (DDP)**

**Concept:**  
Replicate model across processes, each handling unique data shards.

**Implementation Steps:**
- Initialize process group
- Wrap model with `DistributedDataParallel`
- Use `DistributedSampler` for balanced dataset splits
- Synchronize gradients via all-reduce

**Benefits:**
- Near-linear scaling at smaller CPU counts
- No model architecture changes needed

---

### **2ï¸âƒ£ Model Parallelism**

**Concept:**  
Split model layers across multiple CPU devices.

**Device Allocation Example:**
- Device 0: Embedding, FC
- Device 1: LSTM

**Benefits:**
- Better utilization of multiple devices
- Reduces load on single CPU
- Useful for large models

**Trade-offs:**
- Higher communication overhead
- Less efficient if model fits comfortably on one CPU

---

## ğŸ“Š Results & Analysis

### **DDP Performance**
| CPUs | Training Time (min) |
|------|----------------------|
| 8    | 263                  |
| 16   | ~180                 |
| 24   | ~135                 |
| 28   | Higher due to overhead |

**Insights:**
- Training time nearly **cuts in half** from 8 â†’ 24 CPUs
- Best speedup: **1.95Ã—**
- Efficiency decreases with CPU count due to synchronization costs

---

### **DDP + Model Parallel Performance**
- Best performance at **16 CPUs**
- Unstable results at **20 CPUs** due to communication overhead
- Peak speedup: **1.82Ã—**
- Slightly better efficiency at high CPU counts compared to DDP-alone

---

### **Data Preprocessing Performance**
| Tool   | Time (sec) |
|--------|------------|
| Dask   | ~30        |
| Pandas | ~80        |

Dask demonstrates **~3Ã— faster** preprocessing on large files.

---

## Usage

### Serial Training (Baseline)

**CPU Serial Training:** `jupyter notebook SerialProcessing/cpu/SerialExecutionCPU.ipynb`

**GPU Serial Training:** `jupyter notebook SerialProcessing/gpu/SerialExecutionGPU.ipynb`

### Parallel Training

**DDP CPU Training:** 
- `cd ParallelProcessing/cpus_with_DDP/`
- `python main.py --epochs 20 --batch-size 64`

**DDP GPU Training:** 
- `cd ParallelProcessing/gpus_with_DDP/`
- `python main.py --epochs 20 --batch-size 128`

**Full Parallelism (DDP + AMP + Model Parallel):** 
- `cd ParallelProcessing/gpus_with_DDP_AMP_ModelParallel/`
- `python main.py --epochs 20 --amp --model-parallel`

### Analysis

**Performance Analysis:**
- `jupyter notebook Analysis/CPU-Comparison.ipynb`
- `jupyter notebook Analysis/GPU-Comparison.ipynb`
- `jupyter notebook "SpeedUp and Efficiency.ipynb"`

### Advanced Usage

**Custom Parameters:** `python main.py --epochs 50 --batch-size 256 --lr 0.001`

**Mixed Precision:** `python main.py --epochs 20 --amp --gradient-accumulation-steps 4`

**Distributed Launch:** `python -m torch.distributed.launch --nproc_per_node=4 main.py --epochs 20`

**Data Preprocessing:** `python dataset/preprocessing_pipeline.py --input dataset/main_data.csv --output dataset/processed_data.csv`

**Model Evaluation:** `python evaluate.py --model-path models/best_model.pth --test-data dataset/subset_data.csv`

**TensorBoard Monitoring:** `tensorboard --logdir logs/ --port 6006`

## ğŸ“ Repository Structure

```plaintext
Sentiment-Analysis-using-LSTM/
â”‚
â”œâ”€â”€ POC notebooks/
â”‚   â”œâ”€â”€ initial_model_exploration.ipynb
â”‚   â”œâ”€â”€ data_preprocessing_tests.ipynb
â”‚   â”œâ”€â”€ model_architecture_comparison.ipynb
â”‚   â””â”€â”€ baseline_performance.ipynb
â”‚
â”œâ”€â”€ data_and_model_parallel/
â”‚   â”œâ”€â”€ train_hybrid_parallel.py
â”‚   â”œâ”€â”€ model_parallel_lstm.py
â”‚   â”œâ”€â”€ distributed_data_loader.py
â”‚   â”œâ”€â”€ hybrid_performance_analysis.ipynb
â”‚   â”œâ”€â”€ model.py
â”‚   â””â”€â”€ logs/models/metrics/plots/
â”‚
â”œâ”€â”€ data_parallel/
â”‚   â”œâ”€â”€ ddp_training.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ distributed_sampler.py
â”‚   â”œâ”€â”€ scaling_analysis.ipynb
â”‚   â””â”€â”€ logs/models/metrics/plots/
â”‚
â”œâ”€â”€ data_parallel_and_AMP/
â”‚   â”œâ”€â”€ amp_ddp_training.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ gradient_scaler.py
â”‚   â”œâ”€â”€ memory_usage_analysis.ipynb
â”‚   â””â”€â”€ logs/models/metrics/plots/
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ main_data.csv
â”‚   â”œâ”€â”€ subset_data.csv
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ preprocessing_pipeline.py
â”‚   â”œâ”€â”€ text_cleaning.py
â”‚   â”œâ”€â”€ label_encoding.py
â”‚   â””â”€â”€ dataset_info.json
â”‚
â”œâ”€â”€ SerialProcessing/
â”‚   â”œâ”€â”€ cpu/
â”‚   â”‚   â”œâ”€â”€ SerialExecutionCPU.ipynb
â”‚   â”‚   â”œâ”€â”€ single_thread_lstm.py
â”‚   â”‚   â””â”€â”€ logs/models/metrics/plots/
â”‚   â”‚
â”‚   â””â”€â”€ gpu/
â”‚       â”œâ”€â”€ SerialExecutionGPU.ipynb
â”‚       â”œâ”€â”€ SerialExecutionGPU-BatchSize.ipynb
â”‚       â”œâ”€â”€ single_gpu_lstm.py
â”‚       â””â”€â”€ logs/models/metrics/plots/
â”‚
â”œâ”€â”€ ParallelProcessing/
â”‚   â”œâ”€â”€ cpus_with_DDP/
â”‚   â”‚   â”œâ”€â”€ ddp_train.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ ParallelExecutionCPU.ipynb
â”‚   â”‚   â””â”€â”€ logs/models/metrics/plots/
â”‚   â”‚
â”‚   â”œâ”€â”€ gpus_with_DDP/
â”‚   â”‚   â”œâ”€â”€ ddp_train.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ ParallelExecutionGPU.ipynb
â”‚   â”‚   â””â”€â”€ logs/models/metrics/plots/
â”‚   â”‚
â”‚   â”œâ”€â”€ cpus_with_DDP_AMP/
â”‚   â”‚   â”œâ”€â”€ ddp_train.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ ParallelExecutionCPU_AMP.ipynb
â”‚   â”‚   â””â”€â”€ logs/models/metrics/plots/
â”‚   â”‚
â”‚   â””â”€â”€ gpus_with_DDP_AMP_ModelParallel/
â”‚       â”œâ”€â”€ ddp_train.py
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ model.py
â”‚       â”œâ”€â”€ FullParallelExecution.ipynb
â”‚       â””â”€â”€ logs/models/metrics/plots/
â”‚
â”œâ”€â”€ Analysis/
â”‚   â”œâ”€â”€ CPU-Comparison.ipynb
â”‚   â”œâ”€â”€ GPU-Comparison.ipynb
â”‚   â””â”€â”€ Scalability-Analysis.ipynb
â”‚
â”œâ”€â”€ preprocessed_sentiment_data/
â”‚
â”œâ”€â”€ Code Structure.txt
â”œâ”€â”€ EDA and Data Analysis.ipynb
â”œâ”€â”€ SpeedUp and Efficiency.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
